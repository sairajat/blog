{"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2025/06/07/hello-world/"},{"title":"üîç Know thy GPU - A Fun Dive into CUDA Device Introspection","text":"Ever wondered what your GPU is made of? I don‚Äôt mean physically (though that would make a great teardown video) ‚Äî I mean capability-wise. If you‚Äôre working with CUDA, it‚Äôs crucial to know whether your GPU supports managed memory, tensor cores, or concurrent kernel execution. And hey, maybe you‚Äôre just trying to settle a bet about whose card is faster. üèéÔ∏è In this post, we‚Äôll go on a quick and entertaining tour through a powerful C++ tool that queries all your CUDA-capable GPUs and tells you everything from warp size to peak memory bandwidth. Buckle up! üß≠ What We‚Äôll DoWe‚Äôll walk through a simple (but mighty!) C++ program that: Detects all CUDA GPUs on your machine Prints detailed properties like compute capability, memory specs, and core clock Tells you if your GPU can juggle multiple tasks like a caffeinated octopus üêô All this, using cudaDeviceProp and a sprinkle of std::cout magic. üíª The Full Code12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273#include &lt;cuda_runtime.h&gt;#include &lt;iostream&gt;#include &lt;iomanip&gt;int main() { int nDevices; cudaGetDeviceCount(&amp;nDevices); for (int i = 0; i &lt; nDevices; i++) { cudaDeviceProp prop; cudaGetDeviceProperties(&amp;prop, i); std::cout &lt;&lt; &quot;====================================\\n&quot;; std::cout &lt;&lt; &quot;Device Number: &quot; &lt;&lt; i &quot; of &quot; &lt;&lt; nDevices &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Device name: &quot; &lt;&lt; prop.name &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Compute Capability: &quot; &lt;&lt; prop.major &lt;&lt; &quot;.&quot; &lt;&lt; prop.minor &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Streaming Multiprocessors (SMs): &quot; &lt;&lt; prop.multiProcessorCount &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Tensor Cores Available: &quot; &lt;&lt; (prop.major &gt;= 7 ? &quot;yes&quot; : &quot;no&quot;) &lt;&lt; std::endl; std::cout &lt;&lt; std::fixed &lt;&lt; std::setprecision(2); std::cout &lt;&lt; &quot; Clock Rate (Core MHz): &quot; &lt;&lt; prop.clockRate / 1000.0 &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Multiprocessor Clock Rate (MHz): &quot; &lt;&lt; prop.clockRate / 1000.0 &lt;&lt; std::endl; std::cout &lt;&lt; std::setprecision(0); std::cout &lt;&lt; &quot; Memory Clock Rate (MHz): &quot; &lt;&lt; prop.memoryClockRate / 1024 &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Memory Bus Width (bits): &quot; &lt;&lt; prop.memoryBusWidth &lt;&lt; std::endl; std::cout &lt;&lt; std::fixed &lt;&lt; std::setprecision(1); double bandwidth = 2.0 * prop.memoryClockRate * (prop.memoryBusWidth / 8) / 1.0e6; std::cout &lt;&lt; &quot; Peak Memory Bandwidth (GB/s): &quot; &lt;&lt; bandwidth &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Total global memory (Gbytes): &quot; &lt;&lt; static_cast&lt;float&gt;(prop.totalGlobalMem) / (1024 * 1024 * 1024) &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Total constant memory (Kbytes): &quot; &lt;&lt; static_cast&lt;float&gt;(prop.totalConstMem) / 1024.0 &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Shared memory per block (Kbytes): &quot; &lt;&lt; static_cast&lt;float&gt;(prop.sharedMemPerBlock) / 1024.0 &lt;&lt; std::endl; std::cout &lt;&lt; &quot; L2 Cache Size (Kbytes): &quot; &lt;&lt; static_cast&lt;float&gt;(prop.l2CacheSize) / 1024.0 &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Registers per block: &quot; &lt;&lt; prop.regsPerBlock &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Warp size: &quot; &lt;&lt; prop.warpSize &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Max threads per block: &quot; &lt;&lt; prop.maxThreadsPerBlock &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Max threads dim: (&quot; &lt;&lt; prop.maxThreadsDim[0] &lt;&lt; &quot;, &quot; &lt;&lt; prop.maxThreadsDim[1] &lt;&lt; &quot;, &quot; &lt;&lt; prop.maxThreadsDim[2] &lt;&lt; &quot;)\\n&quot;; std::cout &lt;&lt; &quot; Max grid size: (&quot; &lt;&lt; prop.maxGridSize[0] &lt;&lt; &quot;, &quot; &lt;&lt; prop.maxGridSize[1] &lt;&lt; &quot;, &quot; &lt;&lt; prop.maxGridSize[2] &lt;&lt; &quot;)\\n&quot;; std::cout &lt;&lt; &quot; Async Engines: &quot; &lt;&lt; prop.asyncEngineCount &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Concurrent Kernels: &quot; &lt;&lt; (prop.concurrentKernels ? &quot;yes&quot; : &quot;no&quot;) &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Concurrent Copy and Execution: &quot; &lt;&lt; (prop.deviceOverlap ? &quot;yes&quot; : &quot;no&quot;) &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Unified Addressing: &quot; &lt;&lt; (prop.unifiedAddressing ? &quot;yes&quot; : &quot;no&quot;) &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Managed Memory Supported: &quot; &lt;&lt; (prop.managedMemory ? &quot;yes&quot; : &quot;no&quot;) &lt;&lt; std::endl; std::cout &lt;&lt; &quot; PCI Bus ID / Device ID: &quot; &lt;&lt; prop.pciBusID &lt;&lt; &quot; / &quot; &lt;&lt; prop.pciDeviceID &lt;&lt; std::endl; std::cout &lt;&lt; &quot;====================================\\n&quot; &lt;&lt; std::endl; } return 0;} üîé Why This MattersUnderstanding your GPU‚Äôs hardware capabilities is like knowing your car‚Äôs horsepower before entering a drag race. It tells you: Whether you can use advanced CUDA features like Unified Memory or Tensor Cores How much parallelism you can exploit (SMs, warps, threads) If your hardware is limiting your algorithm‚Äôs performance (e.g., low memory bandwidth) What optimization knobs you can safely ignore or push harder It‚Äôs not just about geeking out (although that‚Äôs half the fun) ‚Äî it‚Äôs about writing better, faster GPU code. üß† ConclusionWith just a few lines of C++ and CUDA runtime API, you now have a powerful utility to peek under the hood of any GPU. This kind of introspection is essential when tuning performance or building systems that must adapt to the GPU they run on. So next time someone says, ‚ÄúMy GPU is faster,‚Äù you can pull out this program and say, ‚ÄúProve it.‚Äù Happy hacking! üöÄ","link":"/2025/06/10/know-thy-gpu/"}],"tags":[],"categories":[],"pages":[]}