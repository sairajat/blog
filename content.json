{"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2025/06/07/hello-world/"},{"title":"üîç Know thy GPU - A Fun Dive into CUDA Device Introspection","text":"Ever wondered what your GPU is made of? I don‚Äôt mean physically (though that would make a great teardown video) ‚Äî I mean capability-wise. If you‚Äôre working with CUDA, it‚Äôs crucial to know whether your GPU supports managed memory, tensor cores, or concurrent kernel execution. And hey, maybe you‚Äôre just trying to settle a bet about whose card is faster. üèéÔ∏è In this post, we‚Äôll go on a quick and entertaining tour through a powerful C++ tool that queries all your CUDA-capable GPUs and tells you everything from warp size to peak memory bandwidth. Buckle up! üß≠ What We‚Äôll DoWe‚Äôll walk through a simple (but mighty!) C++ program that: Detects all CUDA GPUs on your machine Prints detailed properties like compute capability, memory specs, and core clock Tells you if your GPU can juggle multiple tasks like a caffeinated octopus üêô All this, using cudaDeviceProp and a sprinkle of std::cout magic. üíª The Full Code12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273#include &lt;cuda_runtime.h&gt;#include &lt;iostream&gt;#include &lt;iomanip&gt;int main() { int nDevices; cudaGetDeviceCount(&amp;nDevices); for (int i = 0; i &lt; nDevices; i++) { cudaDeviceProp prop; cudaGetDeviceProperties(&amp;prop, i); std::cout &lt;&lt; &quot;====================================\\n&quot;; std::cout &lt;&lt; &quot;Device Number: &quot; &lt;&lt; i &quot; of &quot; &lt;&lt; nDevices &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Device name: &quot; &lt;&lt; prop.name &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Compute Capability: &quot; &lt;&lt; prop.major &lt;&lt; &quot;.&quot; &lt;&lt; prop.minor &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Streaming Multiprocessors (SMs): &quot; &lt;&lt; prop.multiProcessorCount &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Tensor Cores Available: &quot; &lt;&lt; (prop.major &gt;= 7 ? &quot;yes&quot; : &quot;no&quot;) &lt;&lt; std::endl; std::cout &lt;&lt; std::fixed &lt;&lt; std::setprecision(2); std::cout &lt;&lt; &quot; Clock Rate (Core MHz): &quot; &lt;&lt; prop.clockRate / 1000.0 &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Multiprocessor Clock Rate (MHz): &quot; &lt;&lt; prop.clockRate / 1000.0 &lt;&lt; std::endl; std::cout &lt;&lt; std::setprecision(0); std::cout &lt;&lt; &quot; Memory Clock Rate (MHz): &quot; &lt;&lt; prop.memoryClockRate / 1024 &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Memory Bus Width (bits): &quot; &lt;&lt; prop.memoryBusWidth &lt;&lt; std::endl; std::cout &lt;&lt; std::fixed &lt;&lt; std::setprecision(1); double bandwidth = 2.0 * prop.memoryClockRate * (prop.memoryBusWidth / 8) / 1.0e6; std::cout &lt;&lt; &quot; Peak Memory Bandwidth (GB/s): &quot; &lt;&lt; bandwidth &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Total global memory (Gbytes): &quot; &lt;&lt; static_cast&lt;float&gt;(prop.totalGlobalMem) / (1024 * 1024 * 1024) &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Total constant memory (Kbytes): &quot; &lt;&lt; static_cast&lt;float&gt;(prop.totalConstMem) / 1024.0 &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Shared memory per block (Kbytes): &quot; &lt;&lt; static_cast&lt;float&gt;(prop.sharedMemPerBlock) / 1024.0 &lt;&lt; std::endl; std::cout &lt;&lt; &quot; L2 Cache Size (Kbytes): &quot; &lt;&lt; static_cast&lt;float&gt;(prop.l2CacheSize) / 1024.0 &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Registers per block: &quot; &lt;&lt; prop.regsPerBlock &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Warp size: &quot; &lt;&lt; prop.warpSize &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Max threads per block: &quot; &lt;&lt; prop.maxThreadsPerBlock &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Max threads dim: (&quot; &lt;&lt; prop.maxThreadsDim[0] &lt;&lt; &quot;, &quot; &lt;&lt; prop.maxThreadsDim[1] &lt;&lt; &quot;, &quot; &lt;&lt; prop.maxThreadsDim[2] &lt;&lt; &quot;)\\n&quot;; std::cout &lt;&lt; &quot; Max grid size: (&quot; &lt;&lt; prop.maxGridSize[0] &lt;&lt; &quot;, &quot; &lt;&lt; prop.maxGridSize[1] &lt;&lt; &quot;, &quot; &lt;&lt; prop.maxGridSize[2] &lt;&lt; &quot;)\\n&quot;; std::cout &lt;&lt; &quot; Async Engines: &quot; &lt;&lt; prop.asyncEngineCount &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Concurrent Kernels: &quot; &lt;&lt; (prop.concurrentKernels ? &quot;yes&quot; : &quot;no&quot;) &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Concurrent Copy and Execution: &quot; &lt;&lt; (prop.deviceOverlap ? &quot;yes&quot; : &quot;no&quot;) &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Unified Addressing: &quot; &lt;&lt; (prop.unifiedAddressing ? &quot;yes&quot; : &quot;no&quot;) &lt;&lt; std::endl; std::cout &lt;&lt; &quot; Managed Memory Supported: &quot; &lt;&lt; (prop.managedMemory ? &quot;yes&quot; : &quot;no&quot;) &lt;&lt; std::endl; std::cout &lt;&lt; &quot; PCI Bus ID / Device ID: &quot; &lt;&lt; prop.pciBusID &lt;&lt; &quot; / &quot; &lt;&lt; prop.pciDeviceID &lt;&lt; std::endl; std::cout &lt;&lt; &quot;====================================\\n&quot; &lt;&lt; std::endl; } return 0;} üîé Why This MattersUnderstanding your GPU‚Äôs hardware capabilities is like knowing your car‚Äôs horsepower before entering a drag race. It tells you: Whether you can use advanced CUDA features like Unified Memory or Tensor Cores How much parallelism you can exploit (SMs, warps, threads) If your hardware is limiting your algorithm‚Äôs performance (e.g., low memory bandwidth) What optimization knobs you can safely ignore or push harder It‚Äôs not just about geeking out (although that‚Äôs half the fun) ‚Äî it‚Äôs about writing better, faster GPU code. üß† ConclusionWith just a few lines of C++ and CUDA runtime API, you now have a powerful utility to peek under the hood of any GPU. This kind of introspection is essential when tuning performance or building systems that must adapt to the GPU they run on. So next time someone says, ‚ÄúMy GPU is faster,‚Äù you can pull out this program and say, ‚ÄúProve it.‚Äù Happy hacking! üöÄ","link":"/2025/06/10/know-thy-gpu/"},{"title":"üß† When Lazy Kernels Hang - A Quirky Tale of CUDA, Streams, and Warmups","text":"Summary:Ever had your CUDA kernels mysteriously hang, even though everything looked fine? You‚Äôre not alone. This post walks through a deceptively simple code snippet that deadlocks ‚Äî and explains how lazy loading, asynchronous streams, and cold GPUs all conspire to make benchmarking and debugging‚Ä¶ interesting. We‚Äôll break down what happens, why it matters, and how to keep your GPU pipelines warm and humming. üí• The Problem: A Hanging CUDA ProgramLet‚Äôs jump straight into the puzzle. Here‚Äôs a CUDA C++ program that may either hang or complete depending on which kernel variant you choose: 123456789101112131415161718192021222324252627282930313233343536#include &lt;iostream&gt;__device__ volatile int s = 0;__global__ void k1(){ while (s == 0) {};}__global__ void k2(){ s = 1;}__global__ void k(int x) { if (x == 0) { while (s == 0) {}; } else { s = 1; }}int main() { cudaStream_t s1, s2; cudaStreamCreate(&amp;s1); cudaStreamCreate(&amp;s2);#if 1 //!!!hang k1&lt;&lt;&lt;1,1,0,s1&gt;&gt;&gt;(); k2&lt;&lt;&lt;1,1,0,s2&gt;&gt;&gt;();#else // works k&lt;&lt;&lt;1,1,0,s1&gt;&gt;&gt;(0); k&lt;&lt;&lt;1,1,0,s2&gt;&gt;&gt;(1);#endif cudaDeviceSynchronize(); cudaError_t err = cudaGetLastError(); if (err != cudaSuccess) std::cout &lt;&lt; cudaGetErrorString(err) &lt;&lt; std::endl;} üß™ What‚Äôs Going On?Let‚Äôs break it down: k1 is a kernel that spins in a loop until s == 1. k2 sets s = 1. Both are launched in different streams (s1 and s2), so they can execute concurrently. But here‚Äôs the twist: this code hangs in the first case, but not in the second. Why? Because of lazy kernel loading. üê¢ Lazy Loading: CUDA‚Äôs Optimization TrickBy default, CUDA uses lazy loading (CUDA_MODULE_LOADING=LAZY). This means: Kernels aren‚Äôt actually loaded onto the GPU until just before they run. Loading kernels might require context synchronization - If a kernel is blocked (e.g., k1 is spinning forever waiting on s), it may never yield, so k2‚Äôs module never gets loaded. Result: k2 never executes ‚Üí s never becomes 1 ‚Üí k1 spins forever ‚Üí üíÄ deadlock. ‚úÖ Fix It: Set Module Loading to EagerTo avoid this, set: 1export CUDA_MODULE_LOADING=EAGER This loads all kernels up front, ensuring both k1 and k2 are resident on the GPU before either begins execution. üî• Why Warmups Matter: The Hidden Complexity of GPU Power StatesThe module loading behavior isn‚Äôt the only reason to warm up your GPU before benchmarking. There‚Äôs a deeper, more hardware-level reason involving GPU power states. Here‚Äôs what happens when your GPU has been sitting idle: It enters a low-power state ‚Äî sometimes almost completely powered down. Components like the memory subsystem, caches, clocks, and even compute cores may be shut off. Bringing the GPU back up is a complex orchestration: Power up voltage rails Wake up clock generators Initialize memory controllers, pin drivers, DRAM Perform ECC scrub (initialize memory with ECC tags) This process takes time ‚Äî seconds in some cases. So your first CUDA call isn‚Äôt benchmarking your kernel; it‚Äôs measuring hardware wake-up time. üëÅÔ∏è How to Observe Power States Use nvidia-smi to see GPU power state (P0 = max performance, P8 = idle). Warning: Running nvidia-smi itself may change the power state. Sneaky. üß† Other Reasons to Warm Up Your Kernels JIT Compilation: CUDA may compile kernels on-the-fly the first time you call them. Page Faults: Unified memory may need to fault and allocate actual device memory. Memory Pooling: Allocators may initialize memory pools only after the first allocation. Clock Boosting: GPU frequency scaling may take a few seconds to reach peak clock. üí° Best Practices for Benchmarking Always run a few dummy kernel launches before recording performance. Explicitly set CUDA_MODULE_LOADING=EAGER for critical benchmarking. Use cudaDeviceSynchronize() after warmups to make sure everything is fully initialized. Pin memory ahead of time to avoid host-to-device delays on first transfer. üéØ ConclusionThat innocent-looking while (s == 0) loop just taught us some deep truths: CUDA uses lazy loading that can lead to hangs if you‚Äôre not careful. GPUs sleep ‚Äî and waking them up is not instant coffee. Benchmarking isn‚Äôt just about timing kernels; it‚Äôs about ensuring a consistent environment. So next time your kernel runs ‚Äúslow‚Äù the first time, don‚Äôt blame the compiler ‚Äî it might just be your GPU stretching and yawning after a nap. üò¥","link":"/2025/06/10/lazy_kernes/"}],"tags":[],"categories":[],"pages":[]}